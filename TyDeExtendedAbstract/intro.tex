\section{Introduction}

As \citet{Dijkstra1988} argues, formal methods form a critical part of computer programming.
The digital nature of the computer means that even a slight error in a program results not in a slight error in output, but entirely unexpected output.
Formal proof forces the programmer to reason through why he believes his program will do what he intends, and using a proof assistant forces this reasoning to be thorough enough that each step can be justified by some formal system.

The family of proof assistants under consideration of this thesis are those based on a dependent type theory (e.g. Agda, Coq, Lean).
These systems have an advantage over others, as programs and proofs are written in a common language \citep{Nordstrom1990}.

For any proof assistant, there is a problem akin to ``Who watches the watchmen?", how does one know that the proof checker is itself correct?
As a solution to this problem, \citet{Barendregt2005} propose that proof assistants should admit the checking of a proof by a small, independent program (commonly referred to as the ``de Bruijn criterion").
By their logic, we can trust a proof assistant whose proof checker is simple enough for us to understand, and which allows us to implement our own proof checker if we aren't convinced existing implementations are correct.

Simplicity isn't all there is to a proof checker, however.
As \citet{Geuvers2008} argues, if the proof checker takes longer than is reasonable to wait, not only is the proof assistant not much use, there can be situations wherein the status of the ``proof" as a proof come into question.
Therefore, the proof checker must be both simple and reasonably efficient (we take ``reasonably efficient" here to mean ``at least as fast as the others").

Accepting this, one might then ask if a dependently typed proof assistant can satisfy this criterion.
To assess this, we must first know what it takes to check a proof in a dependent type theory, and even more basically, what a dependent type theory is.

\subsection{Dependent Types}
\label{sec:dependent-types}

Just as functional languages treat functions as first class values, dependently typed languages treat types as first class values.
An example is the following program, 

\begin{verbatim}
int_or_string : (b : Bool) -> (if b then Int else String)
int_or_string = \b -> if b then 3141 else "Hello, World!"
\end{verbatim}

Here the function \verb|int_or_string| has domain \texttt{Bool}, but its codomain depends on which boolean it is passed.
If it is passed the boolean \texttt{true}, then it will return an \texttt{Int}, specifically \texttt{3141}.
Otherwise if it is passed \texttt{false}, then it will return the \texttt{String "Hello, World!"}.

As an example of the theorem proving capabilities of this language we could prove the theorem \verb|int_or_string(true) = 3141|.

\begin{verbatim}
int_or_string_theorem : int_or_string(true) = 3141
int_or_string_theorem = reflexivity
\end{verbatim}

Here we define a new variable \verb|int_or_string_theorem| whose \textit{type} is \\ \verb|int_or_string(true) = 3141|.
Just like how function types are inhabited by functions, or the integer type is inhabited by integers, the equality types are inhabited by proof of the equality.
Here the proof of equality we have defined \verb|int_or_string_theorem| to be is simply reflexivity, meaning that both sides of the equality are already the same.
This exhibits one of the more confusing aspects of dependent type theory, equality is not syntactic but up to symbolic execution.
Two expressions or types are equal if they can be ``run" to the same value.

In fact, we had already seen this in the definition of \verb|int_or_string|.
One of the judgments of the formal system underlying dependent type theory is the typing judgment taking the form $\Gamma \vdash e : A$, meaning expression $e$ has type $A$ with free variables contained in $\Gamma$.
So in the true branch of \verb|int_or_string| we make the judgment $b : \verb|Bool| \vdash 3141 : (\verb|if true then Int else String|)$, and since we are reasoning up to symbolic execution this is equal to to $b : \verb|Bool| \vdash 3141 : \verb|Int|$.

This ``reasoning up to symbolic execution" is formalized using the second judgment of dependent type theory, the equality judgment.
This takes the form $\Gamma \vdash a = b : A$, meaning $a$ is judgmentally equal to $b$ at type $A$ with free variables $\Gamma$.
The two judgments then interact with each other with the following deduction rule

\begin{mathpar}
  \inferrule*[left=Conv]{\Gamma \vdash e : B \\ \Gamma \vdash A = B : \texttt{Type}}{\Gamma \vdash e : A}
\end{mathpar}

Which states that an expression $e$ of type $B$ can also be considered of type $A$ so long as $A$ and $B$ are judgmentally equal types.

We've already seen two instances of this equality judgment at play in the \verb|int_or_string| and \verb|int_or_string_theorem| example.

\begin{mathpar}
  \inferrule*[left=App-$\beta$]
    {
    }
    {\Gamma \vdash (\text{\textbackslash} x \to b) a = \subst{b}{x}{a} : A
    }

  \inferrule*[left=if-true-$\beta$]
    {
    }
    {\Gamma \vdash \text{if true then } a \text{ else } b = a : A
    }
\end{mathpar}

App-$\beta$ is the familiar lambda calculus $\beta$ rule, where a function literal applied to an argument is equal to the body of the function with the argument substituted for the parameter variable.
This rule was used when type checking \verb|int_or_string_theorem| to judge that \verb|int_or_string(true)| is equal to \verb|if true then 3141 else "Hello, World!"|.
The other rule, if-true-$\beta$, was then used to judge that \verb|if true then 3141 else "Hello, World!"| is equal to \verb|3141|.
Since both these rules capture the execution of a program, we group them together and call them both $\beta$ rules.
We then call an expression which looks like the left-hand side of a $\beta$ rule a ``redex", for reducible expression.

There is, however, another group of rules called $\eta$ rules.
Instead of capturing the execution of a program, they give us additional equalities that we can glean from the type of a term.

\begin{mathpar}
  \inferrule*[left=Fun-$\eta$]
  { \Gamma, x : A \vdash f x = g x : B
  }
  { \Gamma \vdash f = g : (x : A) \to B
  }

  \inferrule*[left=Unit-$\eta$]
  {
  }
  { \Gamma \vdash a = b : \text{Unit}
  }
\end{mathpar}


The most familiar of these is the Fun-$\eta$ rule, which gives us the function extensionality principle for terms of a function type.
Another is the Unit-$\eta$ rule for the Unit type which has the single element $\unitE$.
Since there is only one element of the Unit type, we can judge all expressions of Unit type equal.
This rule lets us type check the following program.

\begin{verbatim}
unit_contractible : (x : Unit) -> (y : Unit) -> x = y
unit_contractible x y = reflexivity
\end{verbatim}

The main part of this thesis will be comparing two systems, one of which supports this Unit-$\eta$ rule and one which doesn't.

\subsection{Implementation}

So, the two main components of a proof checker for a dependent type theory are the algorithms for deciding the typing and equality judgments.
Therefore if our proof assistant based on this theory is to satisfy the de Bruijn criterion, there must be simple, efficient algorithms for deciding these judgments.

When writing code which decides if a judgment is provable for the subjects of the judgment, there is a decision to be made about which subjects will be treated as input and which as output.
For example, in a language with complete type inference (e.g. Haskell, OCaml), the type in the typing judgment is always treated as output while the context and expression are input.

\subsubsection{Typing Judgement}

The Pfenning bidirectional type checking recipe \citep{Dunfield2021} provides a simple algorithm for deciding the typing judgment by carefully choosing when to switch between the input and output modes.
This recipe splits the operators in the language into two camps, constructors which build an element of a specific type and whose typing rule takes the type as input, and destructors which perform some operation on elements of a specific type and whose typing rule produce their type as output.
In the example code given above, \verb|\b -> ...|, \verb|3141|, and \verb|"Hello, World!"| are constructors while the \verb|if| operator is a destructor.
Following this recipe makes the code deciding the typing judgment very regular, making it both simple to read and simple to write.
Additionally, the algorithm performs a single traversal of the syntax tree, making it reasonably efficient.

\subsubsection{Equality Judgement}

The story for the equality judgment, however, is not so straightforward.
As \citet{Abel2013} shows, the most widely used approach for deciding judgmental equality is normalization by evaluation (NbE).
An algorithm for deciding judgmental equality is said to use NbE when it maps the syntax of a language into a semantic domain and then back into syntax.
Since judgmentally equal pieces of syntax will actually be equal in the semantic domain, going there, then back into syntax makes them syntactically equal (a process referred to as ``normalization").
NbE uses this to turn the problem of deciding judgmental equality into one of giving semantics to syntax and then deciding syntactical equality.

NbE can be much simpler to implement when compared with other algorithms, which may require the notoriously tricky capture avoiding substitution, and it can also be faster than other techniques.
However, the ``can"s here are important.
As described above, NbE isn't specifically a single algorithm, but more of an approach an algorithm can take to decide an equivalence relation.

The simplest NbE algorithms are those based on an environment passing interpreter (henceforth referred to as NbE interpreters) such as \citet{Coquand1996} or \citet{Chapman2005}.
Comparatively, the implementations of other algorithms are either much larger in scale, such as the compiler into bytecode described in \citet{Grgoire2002}, or require the knowledge of very dense mathematics to understand such as \citet{Ahman2013}.
In addition, these interpreters can have performance competitive with the most widely used dependently typed languages, as Andras Kovacs' \textit{smalltt}\footnote{\url{https://github.com/AndrasKovacs/smalltt}} demonstrates.
Therefore, NbE interpreters are the best candidate algorithms for deciding judgmental equality.

However, previous work on NbE interpreters follow an ad-hoc recipe for designing their algorithms.
When adding new language constructs, it is unclear how to add accompanying equality rules.
In contrast, bidirectional type checking provides a systematic recipe for adding typing rules for new language constructs by splitting them up into constructors and destructors.
An algorithm for deciding judgmental equality should also provide a systematic recipe for extending the algorithm to language constructs beyond those considered in its original description.

%% However, previous descriptions of NbE interpreters make different design decisions without a clear picture as to the tradeoffs being made.
%% For instance, \citet{Coquand1996} describes an algorithm which doesn't support inspecting the type at which two expressions are being compared at, whereas \citet{Chapman2005} allows the type to be inspected during the syntactic equality test.
%% Another design decision that can be made is to check if the two expressions being compared are syntactically equal before normalizing them, an approach taken in \textit{smalltt}.
%% Unfortunately, no direct comparison has been made between these different designs, and so their cost, both in implementation and in performance, is unclear.
%% Therefore, when implementing a proof checker for a dependently typed language, it is unclear what tradeoffs are being made when selecting an algorithm to use.

\subsubsection{Type-Directed Rules}

Another problem with NbE interpreters, is if and how they can handle ``type-directed" rules.

When comparing two terms for judgmental equality, after the terms are normalized, most judgmental equality rules can be decided by simply comparing the syntax of the two terms.
There are some rules, however, where it is necessary to inspect the type of the two terms to decide if they are equal.
One example is the Unit-$\eta$ rule shown at the end of \autoref{sec:dependent-types}.

The algorithm described by \citet{Coquand1996} takes the easy way out and excludes these rules from the theory, resulting in a weaker, syntax-directed, judgmental equality.
However, doing so puts the burden of proof on the user of the theory when a type-directed rule is required for a program to type check.

\citet{Chapman2005} describe an algorithm which supports type-directed rules by giving the type of the two normalized terms as an argument to the procedure deciding their equality.
The procedure can then inspect the type when it comes across a situation in which a type-directed rule is applicable.
Another approach is documented by Andras Kovacs in his \textit{elaboration-zoo} \footnote{\url{https://github.com/AndrasKovacs/elaboration-zoo}}.
There, the type of each term is calculated during normalization and then stored with the normalized term, so it can be inspected as needed during the equality check.

Here we have three different approaches where the tradeoffs they make are unclear.
It could be the case that handling type-directed rules causes untenable performance loss, or requires unwieldy implementation, meaning theories which require them should be avoided.
On the other hand, one of the approaches to handling them described above could have minimal performance cost and be quite simple to implement.
The problem is that no comparison between them has been made, and so there is currently no good way to decide between them.

\subsection{Thesis Statement}
We propose that the type-directed algorithm described by \citet{Chapman2005} will perform as well as a purely syntax-directed approach while allowing for theories with a stronger judgmental equality.
The rest of this thesis is split into two parts.
First we define exactly what the syntax and type-directed approaches to judgmental equality are, and how an algorithm for deciding the type-directed equality can be derived from the bidirectional recipe.
Then we present the results of a performance comparison between Kovac's smalltt, which uses a syntax-directed equality, and version of smalltt we modified to use a type-directed equality.

% I theorize that the algorithm described by \citet{Chapman2005} will perform as well as a purely syntax-directed approach while allowing for theories with a stronger, type-directed, judgemental equality.
% This belief is based on previous experience implementing a type checker for a dependently typed language.
% At the point in the type checker where two terms must be checked for judgemental equality, their type has already been calculated.
% Therefore passing that type into the procedure which decides judgemental equality should have minimal overhead.
% 
% Furthermore, I propose a generalization of the ad-hoc design of this algorithm, to a systematic recipe inspired by bidirectional type checking.
% 
% In general, judgemental equality rules can be split into two categories \citep{MartinLof1975}.
% The first are called $\beta$ rules, and they occur when a destructor and constructor for some type come into contact.
% The prototypical example for this is for functions.
% When the destructor for functions, function application, meets the constructor for functions, lambda expressions, we get the following judgemental equality rule.
% 
% \begin{mathpar}
%   \inferrule*[left=(Fun-$\beta$)]{ }{\Gamma \vdash (\lambda x. b) a = b[x := a] : B}
% \end{mathpar}
% 
% The second category of rules are called $\eta$ rules.
% These rules specify that two elements of a type are equal when applying the destructors for that type to both elements yield equal results.
% The eta rule for functions can be given as the following.
% 
% \begin{mathpar}
%   \inferrule*[left=(Fun-$\eta$)]{\Gamma, x : A \vdash f(x) = f'(x) : B}{\Gamma \vdash f = f' : A \rightarrow B}
% \end{mathpar}
% 
% My proposed systematic recipe, and how previous NbE interpreters have been implicitly designed, is to apply the $\beta$ rules during normalization, then augment the syntactic equality check with the $\eta$ rules.
% 
% \hfill\break
% 
% \textit{smalltt} already implements a pure syntax-directed judgemental equality in the vein of \citet{Coquand1996}.
% So, to test my claim that my proposed approach retains the performance of the syntax-directed approach, I will modify \textit{smalltt} to use the algorithm described by \citet{Chapman2005}.
% Then, this modified version will be benchmarked against the original \textit{smalltt} with the existing benchmark suite provided by \textit{smalltt}.
% Additionally, to test my systematic recipe, and demonstrate the ability to decide type-directed rules, dependent sums and the unit type, along with their associated $\beta$ and $\eta$ rules as described in \citet{Chapman2005}, will be added to my modified version of \textit{smalltt}.


