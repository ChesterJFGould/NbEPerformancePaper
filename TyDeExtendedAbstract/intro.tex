\section{Introduction}
Dependently typed languages treat types as first class values, enabling computation in types.
Consider the following function.
{\small
\begin{verbatim}
int_or_string : (b : Bool) -> (if b then Int else String)
int_or_string = \b -> if b then 3141 else "Hello, World!"
\end{verbatim}
}
When applied to \texttt{true}, it will return the \texttt{Int} "3141".
When applied to \texttt{false}, it will return the \texttt{String "Hello, World!"}.
We can even express such facts as types.
{\small
\begin{verbatim}
int_or_string_theorem : int_or_string(true) = 3141
int_or_string_theorem = reflexivity
\end{verbatim}
}

Type checking these requires deciding equality between two expressions.
The typing judgement $\Gamma \vdash e : A$ (expression $e$ has type $A$ with free variables $\Gamma$), must reason about execution (or at least equality) of programs.
In the true branch of \verb|int_or_string| we make the judgement $\vdash 3141 : ({\small\verb|if true then Int else String|})$, and must reason this type is equal {\small\verb|Int|}.

Equality is captured by judgements of the form $\Gamma \vdash a = b : A$ ($a$ is equal to $b$ at type $A$ with free variables $\Gamma$).
The two judgements then interact with each other with the following typing rule.
\begin{mathpar}
  \inferrule*[left=Conv]{\Gamma \vdash e : B \\ \Gamma \vdash A = B : \texttt{Type}}{\Gamma \vdash e : A}
\end{mathpar}
How to implement decisions procedures for judgements of this form is an oft debated topic.

An expression $e$ of type $B$ can also be considered of type $A$ so long as $A$ and $B$ are equal.

Our example relies on the following two rules.
\begin{mathpar}
  \inferrule*[left=App-$\beta$]
    {
    }
    {\Gamma \vdash (\text{\textbackslash} x \to b) a = \subst{b}{x}{a} : A
    }

  \inferrule*[left=if-true-$\beta$]
    {
    }
    {\Gamma \vdash \text{if true then } a \text{ else } b = a : A
    }
\end{mathpar}
Both are $\beta$ rules, which capture the execution of a program.
App-$\beta$ is the familiar $\lambda$-calculus $\beta$ rule, where a function literal applied to an argument is equal to the body of the function with the argument substituted for the parameter variable.
These rules are syntax-directed; we can see the type $A$ is not relevant in either rule. 

Another group of rules, called $\eta$ rules, give us additional equalities that rely on the type, rather than capturing the execution of a program.
\begin{mathpar}
  \inferrule*[left=Fun-$\eta$]
  { \Gamma, x : A \vdash f x = g x : B
  }
  { \Gamma \vdash f = g : (x : A) \to B
  }

  \inferrule*[left=Unit-$\eta$]
  {
  }
  { \Gamma \vdash a = b : \text{Unit}
  }
\end{mathpar}
The Fun-$\eta$ rule gives a function extensionality principle for terms of a function type.
The Unit-$\eta$ rule implies the Unit type has the single element $\unitE$.
Since there is only one element of the Unit type, all expressions of Unit type equal.
This rule lets us type check the following program.
{\small
\begin{verbatim}
unit_contractible : (x : Unit) -> (y : Unit) -> x = y
unit_contractible x y = reflexivity
\end{verbatim}
}

As \citet{Abel2013} shows, a widely used approach for deciding judgemental equality is normalization by evaluation (NbE).
An algorithm for deciding judgemental equality is said to use NbE when it maps the syntax of a language into a semantic domain and then back into syntax.
Since judgementally equal pieces of syntax will actually be equal in the semantic domain, going there, then back into syntax makes them syntactically equal (a process referred to as ``normalization").
NbE uses this to turn the problem of deciding judgemental equality into one of giving semantics to syntax and then deciding syntactic equality.

Simple NbE algorithms implement an environment passing interpreter (henceforth referred to as NbE interpreters) such as \citet{Coquand1996} or \citet{Chapman2005}.
%Other algorithms are either much larger in scale, such as the compiler into bytecode described in \citet{Grgoire2002}, or require the knowledge of very dense mathematics to understand such as \citet{Ahman2013}.
These interpreters can have performance competitive with the widely used dependently typed languages, as \citet{smalltt} demonstrates with smalltt.

%However, previous work on NbE interpreters follow an ad-hoc recipe for designing their algorithms.
%When adding new language constructs, it is unclear how to add accompanying equality rules.
%In contrast, bidirectional type checking provides a systematic recipe for adding typing rules for new language constructs by splitting them up into constructors and destructors.
%An algorithm for deciding judgemental equality should also provide a systematic recipe for extending the algorithm to language constructs beyond those considered in its original description.

%% However, previous descriptions of NbE interpreters make different design decisions without a clear picture as to the tradeoffs being made.
%% For instance, \citet{Coquand1996} describes an algorithm which doesn't support inspecting the type at which two expressions are being compared at, whereas \citet{Chapman2005} allows the type to be inspected during the syntactic equality test.
%% Another design decision that can be made is to check if the two expressions being compared are syntactically equal before normalizing them, an approach taken in \textit{smalltt}.
%% Unfortunately, no direct comparison has been made between these different designs, and so their cost, both in implementation and in performance, is unclear.
%% Therefore, when implementing a proof checker for a dependently typed language, it is unclear what tradeoffs are being made when selecting an algorithm to use.

%\subsubsection{Type-Directed Rules}

However, another design decision remains: how to handle the type-directed $\eta$ rules (if at all).

Claims abound that syntax-directed approaches are more performant than those that integrate type-directed rules.
But how much more performant, and is that performance worth the loss in expressivity?
\todo{Citations for such folklore claims?}

It could be the case that handling type-directed rules causes untenable performance loss, or requires unwieldy implementation, meaning theories which require them should be avoided.
On the other hand, one of the approaches to handling them described above could have minimal performance cost and be quite simple to implement.
The problem is that no apples-to-apples comparison between them has been made, and so there is currently no good way to decide between them.

We would like to change that.

%The algorithm described by \citet{Coquand1996} takes the easy way out and excludes these rules from the theory, resulting in a weaker, syntax-directed, judgemental equality.
%However, doing so puts the burden of proof on the user of the theory when a type-directed rule is required for a program to type check.
%
%\citet{Chapman2005} describe an algorithm which supports type-directed rules by giving the type of the two normalized terms as an argument to the procedure deciding their equality.
%The procedure can then inspect the type when it comes across a situation in which a type-directed rule is applicable.
%Another approach is documented by Andras Kovacs in his \textit{elaboration-zoo} \footnote{\url{https://github.com/AndrasKovacs/elaboration-zoo}}.
%There, the type of each term is calculated during normalization and then stored with the normalized term, so it can be inspected as needed during the equality check.

% I theorize that the algorithm described by \citet{Chapman2005} will perform as well as a purely syntax-directed approach while allowing for theories with a stronger, type-directed, judgemental equality.
% This belief is based on previous experience implementing a type checker for a dependently typed language.
% At the point in the type checker where two terms must be checked for judgemental equality, their type has already been calculated.
% Therefore passing that type into the procedure which decides judgemental equality should have minimal overhead.
% 
% Furthermore, I propose a generalization of the ad-hoc design of this algorithm, to a systematic recipe inspired by bidirectional type checking.
% 
% In general, judgemental equality rules can be split into two categories \citep{MartinLof1975}.
% The first are called $\beta$ rules, and they occur when a destructor and constructor for some type come into contact.
% The prototypical example for this is for functions.
% When the destructor for functions, function application, meets the constructor for functions, lambda expressions, we get the following judgemental equality rule.
% 
% \begin{mathpar}
%   \inferrule*[left=(Fun-$\beta$)]{ }{\Gamma \vdash (\lambda x. b) a = b[x := a] : B}
% \end{mathpar}
% 
% The second category of rules are called $\eta$ rules.
% These rules specify that two elements of a type are equal when applying the destructors for that type to both elements yield equal results.
% The eta rule for functions can be given as the following.
% 
% \begin{mathpar}
%   \inferrule*[left=(Fun-$\eta$)]{\Gamma, x : A \vdash f(x) = f'(x) : B}{\Gamma \vdash f = f' : A \rightarrow B}
% \end{mathpar}
% 
% My proposed systematic recipe, and how previous NbE interpreters have been implicitly designed, is to apply the $\beta$ rules during normalization, then augment the syntactic equality check with the $\eta$ rules.
% 
% \hfill\break
% 
% \textit{smalltt} already implements a pure syntax-directed judgemental equality in the vein of \citet{Coquand1996}.
% So, to test my claim that my proposed approach retains the performance of the syntax-directed approach, I will modify \textit{smalltt} to use the algorithm described by \citet{Chapman2005}.
% Then, this modified version will be benchmarked against the original \textit{smalltt} with the existing benchmark suite provided by \textit{smalltt}.
% Additionally, to test my systematic recipe, and demonstrate the ability to decide type-directed rules, dependent sums and the unit type, along with their associated $\beta$ and $\eta$ rules as described in \citet{Chapman2005}, will be added to my modified version of \textit{smalltt}.


