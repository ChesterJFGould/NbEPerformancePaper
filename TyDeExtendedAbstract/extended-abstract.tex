\documentclass[sigplan,nonacm]{acmart}

\usepackage{natbib}
\usepackage{url}
\usepackage{doi}
\bibliographystyle{ACM-Reference-Format}

\usepackage{lipsum}

\title{Extended Abstract: Towards a Performance Comparison of Syntax and Type Directed NbE}

\usepackage{mathpartir}
\usepackage{hyperref}

\usepackage{listings}
\lstset{
  frame=none,
  xleftmargin=2pt,
  % stepnumber=1,
  % numbers=left,
  % numbersep=5pt,
  % numberstyle=\ttfamily\tiny\color[gray]{0.3},
  belowcaptionskip=\bigskipamount,
  captionpos=b,
  escapeinside={*'}{'*},
  language=haskell,
  tabsize=2,
  emphstyle={\bf},
  commentstyle=\it,
  stringstyle=\mdseries\rmfamily,
  showspaces=false,
  keywordstyle=\bfseries\rmfamily,
  columns=flexible,
  basicstyle=\small\sffamily,
  showstringspaces=false,
  morecomment=[l]\%,
  literate={->}{{$\to$}}2 {~}{{$\sim$}}1
}

\newcommand{\bnflabel}[1]{\text{#1}}
\newcommand{\bnfdef}{::=}
\newcommand{\bnfalt}{\mid}
\newcommand{\bnfcomment}[1]{}% \text{\small \textit{#1}}}

\newcommand{\lamE}[2]{\lambda #1. \text{ } #2}
\newcommand{\appE}[2]{#1 \text{ } #2}
\newcommand{\piE}[3]{\Pi #1 : #2. \text{ } #3}
\newcommand{\pairE}[2]{\langle #1, #2 \rangle}
\newcommand{\fstE}[1]{\text{fst } #1}
\newcommand{\sndE}[1]{\text{snd } #1}
\newcommand{\sigmaE}[3]{\Sigma #1 : #2. \text{ } #3}
\newcommand{\unitE}{\langle \rangle}
\newcommand{\unittE}{\text{Unit}}
\newcommand{\univE}{\text{Type}}
\newcommand{\annE}[2]{#1 : #2}

\newcommand{\checkJ}[3]{#1 \vdash #2 \Leftarrow #3}
\newcommand{\synthJ}[3]{#1 \vdash #2 \Rightarrow #3}
\newcommand{\tyEqJ}[4]{#1 \vdash #2 = #3 : #4}
\newcommand{\stxEqJ}[3]{#1 \vdash #2 = #3}
\newcommand{\chkEqJ}[4]{#1 \vdash #2 = #3 \Leftarrow #4}
\newcommand{\synEqJ}[4]{#1 \vdash #2 = #3 \Rightarrow #4}
\newcommand{\freshJ}[2]{#1 \vdash #2 \text{ fresh}}
\newcommand{\steps}[2]{#1 \Downarrow #2}

\newcommand{\subst}[3]{#1 [#2 \mapsto #3]}

\newcommand{\mycomment}[1]{}

\begin{document}

\begin{abstract}
A key part of any dependent type checker is the method for checking whether two types are equal.
A common claim is that syntax-directed equality is more performant, although type-directed equality is more expressive.
However, this claim is difficult to make precise, since implementations choose only one or the other approach, making a direct comparison impossible.
We present some work-in-progress developing a realistic platform for direct, apples-to-apples, comparison of the two approaches, quantifying how much slower type-direct equality checking is, and analyzing why and how it can be improved.
\end{abstract}

\maketitle

\section{Introduction}

As \citet{Dijkstra1988} argues, formal methods form a critical part of computer programming.
The digital nature of the computer means that even a slight error in a program results not in a slight error in output, but entirely unexpected output.
Formal proof forces the programmer to reason through why he believes his program will do what he intends, and using a proof assistant forces this reasoning to be thorough enough that each step can be justified by some formal system.

The family of proof assistants under consideration of this thesis are those based on a dependent type theory (e.g. Agda, Coq, Lean).
These systems have an advantage over others, as programs and proofs are written in a common language \citep{Nordstrom1990}.

For any proof assistant, there is a problem akin to ``Who watches the watchmen?", how does one know that the proof checker is itself correct?
As a solution to this problem, \citet{Barendregt2005} propose that proof assistants should admit the checking of a proof by a small, independent program (commonly referred to as the ``de Bruijn criterion").
By their logic, we can trust a proof assistant whose proof checker is simple enough for us to understand, and which allows us to implement our own proof checker if we aren't convinced existing implementations are correct.

Simplicity isn't all there is to a proof checker, however.
As \citet{Geuvers2008} argues, if the proof checker takes longer than is reasonable to wait, not only is the proof assistant not much use, there can be situations wherein the status of the ``proof" as a proof come into question.
Therefore, the proof checker must be both simple and reasonably efficient (we take ``reasonably efficient" here to mean ``at least as fast as the others").

Accepting this, one might then ask if a dependently typed proof assistant can satisfy this criterion.
To assess this, we must first know what it takes to check a proof in a dependent type theory, and even more basically, what a dependent type theory is.

\subsection{Dependent Types}
\label{sec:dependent-types}

Just as functional languages treat functions as first class values, dependently typed languages treat types as first class values.
An example is the following program, 

\begin{verbatim}
int_or_string : (b : Bool) -> (if b then Int else String)
int_or_string = \b -> if b then 3141 else "Hello, World!"
\end{verbatim}

Here the function \verb|int_or_string| has domain \texttt{Bool}, but its codomain depends on which boolean it is passed.
If it is passed the boolean \texttt{true}, then it will return an \texttt{Int}, specifically \texttt{3141}.
Otherwise if it is passed \texttt{false}, then it will return the \texttt{String "Hello, World!"}.

As an example of the theorem proving capabilities of this language we could prove the theorem \verb|int_or_string(true) = 3141|.

\begin{verbatim}
int_or_string_theorem : int_or_string(true) = 3141
int_or_string_theorem = reflexivity
\end{verbatim}

Here we define a new variable \verb|int_or_string_theorem| whose \textit{type} is \\ \verb|int_or_string(true) = 3141|.
Just like how function types are inhabited by functions, or the integer type is inhabited by integers, the equality types are inhabited by proof of the equality.
Here the proof of equality we have defined \verb|int_or_string_theorem| to be is simply reflexivity, meaning that both sides of the equality are already the same.
This exhibits one of the more confusing aspects of dependent type theory, equality is not syntactic but up to symbolic execution.
Two expressions or types are equal if they can be ``run" to the same value.

In fact, we had already seen this in the definition of \verb|int_or_string|.
One of the judgments of the formal system underlying dependent type theory is the typing judgment taking the form $\Gamma \vdash e : A$, meaning expression $e$ has type $A$ with free variables contained in $\Gamma$.
So in the true branch of \verb|int_or_string| we make the judgment $b : \verb|Bool| \vdash 3141 : (\verb|if true then Int else String|)$, and since we are reasoning up to symbolic execution this is equal to to $b : \verb|Bool| \vdash 3141 : \verb|Int|$.

This ``reasoning up to symbolic execution" is formalized using the second judgment of dependent type theory, the equality judgment.
This takes the form $\Gamma \vdash a = b : A$, meaning $a$ is judgmentally equal to $b$ at type $A$ with free variables $\Gamma$.
The two judgments then interact with each other with the following deduction rule

\begin{mathpar}
  \inferrule*[left=Conv]{\Gamma \vdash e : B \\ \Gamma \vdash A = B : \texttt{Type}}{\Gamma \vdash e : A}
\end{mathpar}

Which states that an expression $e$ of type $B$ can also be considered of type $A$ so long as $A$ and $B$ are judgmentally equal types.

We've already seen two instances of this equality judgment at play in the \verb|int_or_string| and \verb|int_or_string_theorem| example.

\begin{mathpar}
  \inferrule*[left=App-$\beta$]
    {
    }
    {\Gamma \vdash (\text{\textbackslash} x \to b) a = \subst{b}{x}{a} : A
    }

  \inferrule*[left=if-true-$\beta$]
    {
    }
    {\Gamma \vdash \text{if true then } a \text{ else } b = a : A
    }
\end{mathpar}

App-$\beta$ is the familiar lambda calculus $\beta$ rule, where a function literal applied to an argument is equal to the body of the function with the argument substituted for the parameter variable.
This rule was used when type checking \verb|int_or_string_theorem| to judge that \verb|int_or_string(true)| is equal to \verb|if true then 3141 else "Hello, World!"|.
The other rule, if-true-$\beta$, was then used to judge that \verb|if true then 3141 else "Hello, World!"| is equal to \verb|3141|.
Since both these rules capture the execution of a program, we group them together and call them both $\beta$ rules.
We then call an expression which looks like the left-hand side of a $\beta$ rule a ``redex", for reducible expression.

There is, however, another group of rules called $\eta$ rules.
Instead of capturing the execution of a program, they give us additional equalities that we can glean from the type of a term.

\begin{mathpar}
  \inferrule*[left=Fun-$\eta$]
  { \Gamma, x : A \vdash f x = g x : B
  }
  { \Gamma \vdash f = g : (x : A) \to B
  }

  \inferrule*[left=Unit-$\eta$]
  {
  }
  { \Gamma \vdash a = b : \text{Unit}
  }
\end{mathpar}


The most familiar of these is the Fun-$\eta$ rule, which gives us the function extensionality principle for terms of a function type.
Another is the Unit-$\eta$ rule for the Unit type which has the single element $\unitE$.
Since there is only one element of the Unit type, we can judge all expressions of Unit type equal.
This rule lets us type check the following program.

\begin{verbatim}
unit_contractible : (x : Unit) -> (y : Unit) -> x = y
unit_contractible x y = reflexivity
\end{verbatim}

The main part of this thesis will be comparing two systems, one of which supports this Unit-$\eta$ rule and one which doesn't.

\subsection{Implementation}

So, the two main components of a proof checker for a dependent type theory are the algorithms for deciding the typing and equality judgments.
Therefore if our proof assistant based on this theory is to satisfy the de Bruijn criterion, there must be simple, efficient algorithms for deciding these judgments.

When writing code which decides if a judgment is provable for the subjects of the judgment, there is a decision to be made about which subjects will be treated as input and which as output.
For example, in a language with complete type inference (e.g. Haskell, OCaml), the type in the typing judgment is always treated as output while the context and expression are input.

\subsubsection{Typing Judgement}

The Pfenning bidirectional type checking recipe \citep{Dunfield2021} provides a simple algorithm for deciding the typing judgment by carefully choosing when to switch between the input and output modes.
This recipe splits the operators in the language into two camps, constructors which build an element of a specific type and whose typing rule takes the type as input, and destructors which perform some operation on elements of a specific type and whose typing rule produce their type as output.
In the example code given above, \verb|\b -> ...|, \verb|3141|, and \verb|"Hello, World!"| are constructors while the \verb|if| operator is a destructor.
Following this recipe makes the code deciding the typing judgment very regular, making it both simple to read and simple to write.
Additionally, the algorithm performs a single traversal of the syntax tree, making it reasonably efficient.

\subsubsection{Equality Judgement}

The story for the equality judgment, however, is not so straightforward.
As \citet{Abel2013} shows, the most widely used approach for deciding judgmental equality is normalization by evaluation (NbE).
An algorithm for deciding judgmental equality is said to use NbE when it maps the syntax of a language into a semantic domain and then back into syntax.
Since judgmentally equal pieces of syntax will actually be equal in the semantic domain, going there, then back into syntax makes them syntactically equal (a process referred to as ``normalization").
NbE uses this to turn the problem of deciding judgmental equality into one of giving semantics to syntax and then deciding syntactical equality.

NbE can be much simpler to implement when compared with other algorithms, which may require the notoriously tricky capture avoiding substitution, and it can also be faster than other techniques.
However, the ``can"s here are important.
As described above, NbE isn't specifically a single algorithm, but more of an approach an algorithm can take to decide an equivalence relation.

The simplest NbE algorithms are those based on an environment passing interpreter (henceforth referred to as NbE interpreters) such as \citet{Coquand1996} or \citet{Chapman2005}.
Comparatively, the implementations of other algorithms are either much larger in scale, such as the compiler into bytecode described in \citet{Grgoire2002}, or require the knowledge of very dense mathematics to understand such as \citet{Ahman2013}.
In addition, these interpreters can have performance competitive with the most widely used dependently typed languages, as Andras Kovacs' \textit{smalltt}\footnote{\url{https://github.com/AndrasKovacs/smalltt}} demonstrates.
Therefore, NbE interpreters are the best candidate algorithms for deciding judgmental equality.

However, previous work on NbE interpreters follow an ad-hoc recipe for designing their algorithms.
When adding new language constructs, it is unclear how to add accompanying equality rules.
In contrast, bidirectional type checking provides a systematic recipe for adding typing rules for new language constructs by splitting them up into constructors and destructors.
An algorithm for deciding judgmental equality should also provide a systematic recipe for extending the algorithm to language constructs beyond those considered in its original description.

%% However, previous descriptions of NbE interpreters make different design decisions without a clear picture as to the tradeoffs being made.
%% For instance, \citet{Coquand1996} describes an algorithm which doesn't support inspecting the type at which two expressions are being compared at, whereas \citet{Chapman2005} allows the type to be inspected during the syntactic equality test.
%% Another design decision that can be made is to check if the two expressions being compared are syntactically equal before normalizing them, an approach taken in \textit{smalltt}.
%% Unfortunately, no direct comparison has been made between these different designs, and so their cost, both in implementation and in performance, is unclear.
%% Therefore, when implementing a proof checker for a dependently typed language, it is unclear what tradeoffs are being made when selecting an algorithm to use.

\subsubsection{Type-Directed Rules}

Another problem with NbE interpreters, is if and how they can handle ``type-directed" rules.

When comparing two terms for judgmental equality, after the terms are normalized, most judgmental equality rules can be decided by simply comparing the syntax of the two terms.
There are some rules, however, where it is necessary to inspect the type of the two terms to decide if they are equal.
One example is the Unit-$\eta$ rule shown at the end of \autoref{sec:dependent-types}.

The algorithm described by \citet{Coquand1996} takes the easy way out and excludes these rules from the theory, resulting in a weaker, syntax-directed, judgmental equality.
However, doing so puts the burden of proof on the user of the theory when a type-directed rule is required for a program to type check.

\citet{Chapman2005} describe an algorithm which supports type-directed rules by giving the type of the two normalized terms as an argument to the procedure deciding their equality.
The procedure can then inspect the type when it comes across a situation in which a type-directed rule is applicable.
Another approach is documented by Andras Kovacs in his \textit{elaboration-zoo} \footnote{\url{https://github.com/AndrasKovacs/elaboration-zoo}}.
There, the type of each term is calculated during normalization and then stored with the normalized term, so it can be inspected as needed during the equality check.

Here we have three different approaches where the tradeoffs they make are unclear.
It could be the case that handling type-directed rules causes untenable performance loss, or requires unwieldy implementation, meaning theories which require them should be avoided.
On the other hand, one of the approaches to handling them described above could have minimal performance cost and be quite simple to implement.
The problem is that no comparison between them has been made, and so there is currently no good way to decide between them.

\subsection{Thesis Statement}
We propose that the type-directed algorithm described by \citet{Chapman2005} will perform as well as a purely syntax-directed approach while allowing for theories with a stronger judgmental equality.
The rest of this thesis is split into two parts.
First we define exactly what the syntax and type-directed approaches to judgmental equality are, and how an algorithm for deciding the type-directed equality can be derived from the bidirectional recipe.
Then we present the results of a performance comparison between Kovac's smalltt, which uses a syntax-directed equality, and version of smalltt we modified to use a type-directed equality.

% I theorize that the algorithm described by \citet{Chapman2005} will perform as well as a purely syntax-directed approach while allowing for theories with a stronger, type-directed, judgemental equality.
% This belief is based on previous experience implementing a type checker for a dependently typed language.
% At the point in the type checker where two terms must be checked for judgemental equality, their type has already been calculated.
% Therefore passing that type into the procedure which decides judgemental equality should have minimal overhead.
% 
% Furthermore, I propose a generalization of the ad-hoc design of this algorithm, to a systematic recipe inspired by bidirectional type checking.
% 
% In general, judgemental equality rules can be split into two categories \citep{MartinLof1975}.
% The first are called $\beta$ rules, and they occur when a destructor and constructor for some type come into contact.
% The prototypical example for this is for functions.
% When the destructor for functions, function application, meets the constructor for functions, lambda expressions, we get the following judgemental equality rule.
% 
% \begin{mathpar}
%   \inferrule*[left=(Fun-$\beta$)]{ }{\Gamma \vdash (\lambda x. b) a = b[x := a] : B}
% \end{mathpar}
% 
% The second category of rules are called $\eta$ rules.
% These rules specify that two elements of a type are equal when applying the destructors for that type to both elements yield equal results.
% The eta rule for functions can be given as the following.
% 
% \begin{mathpar}
%   \inferrule*[left=(Fun-$\eta$)]{\Gamma, x : A \vdash f(x) = f'(x) : B}{\Gamma \vdash f = f' : A \rightarrow B}
% \end{mathpar}
% 
% My proposed systematic recipe, and how previous NbE interpreters have been implicitly designed, is to apply the $\beta$ rules during normalization, then augment the syntactic equality check with the $\eta$ rules.
% 
% \hfill\break
% 
% \textit{smalltt} already implements a pure syntax-directed judgemental equality in the vein of \citet{Coquand1996}.
% So, to test my claim that my proposed approach retains the performance of the syntax-directed approach, I will modify \textit{smalltt} to use the algorithm described by \citet{Chapman2005}.
% Then, this modified version will be benchmarked against the original \textit{smalltt} with the existing benchmark suite provided by \textit{smalltt}.
% Additionally, to test my systematic recipe, and demonstrate the ability to decide type-directed rules, dependent sums and the unit type, along with their associated $\beta$ and $\eta$ rules as described in \citet{Chapman2005}, will be added to my modified version of \textit{smalltt}.

\section{Main Ideas}

Here we define the syntax and type-directed judgmental equalities, and also demonstrate the close connection between the type-directed equality rules and the bidirectional typing rules.
We first present both the syntax and type-directed equalities for a small dependent type theory in which the two equalities differ only in presentation.
Then, we present an extension to the theory with two new types.
We add the unit type to show that the type-directed equality supports more rules than the syntax-directed equality.
And we add dependent pairs to further demonstrate the connection between the equality and typing rules.
% Then, to show that the type-directed equality supports more rules than the syntax-directed equality, we extend the theory with the unit type.

Our purpose here is to give the reader a rigorous understanding of what these two equalities are and how they are implemented in smalltt.
Therefore, we present the theory in a form very close to that of both smalltt implementations, but we don't prove any theorems about this theory.
That being said, the theory we present is very similar to that proposed in \citet{altenkirch2010}.

\subsection{Syntax and Semantics}

\begin{figure}[!htb]
  \begin{displaymath}
    \begin{array}{lrcll}
      \text{Expressions} & e & \bnfdef & x & \bnfcomment{variables} \\
      & & \bnfalt & \lamE{x}{e} & \bnfcomment{function literal} \\
      & & \bnfalt & \appE{e_f}{e_a} & \bnfcomment{function application} \\
      & & \bnfalt & \piE{x}{e_d}{e_c} & \bnfcomment{dependent function type} \\
      & & \bnfalt & \univE & \bnfcomment{type universe} \\
      & & \bnfalt & \annE{e}{e_t} & \bnfcomment{type annotation} \\
      \\
      \text{Normal Forms} & n & \bnfdef & \lamE{x}{e} & \\
      & & \bnfalt & \piE{x}{n_d}{e_c} & \\
      & & \bnfalt & \univE & \\
      & & \bnfalt & \nu & \\
      \\
      \text{Neutral Terms} & \nu & \bnfdef & x & \\
      & & \bnfalt & \appE{\nu_f}{n_a} & \\
      \\
      \text{Type Contexts} & \Gamma & \bnfdef & \cdot & \\
      & & \bnfalt & \Gamma, x : n & \\
    \end{array}
  \end{displaymath}
  \caption{Syntax}
  \label{fig:base-syntax}
\end{figure}

Before defining the two kinds of judgmental equality, we must first define the theory whose terms we want to judge equal.
The syntax is presented in \autoref{fig:base-syntax}.
It consists of expressions $e$ which are either variables $x$, function introduction or elimination forms, dependent function types, the type universe, or a type annotation.
We also define a subset of expressions called normal forms.
These are expressions which only contain redexes that are inside a binding form.
We will use these to represent the types during type checking since we don't need to reduce them to check if they are type constructors.
For example, if we used expressions to represent types, we could have $\appE{(\lamE{x}{\piE{y}{\unittE}{\unittE}})}{\unitE}$ which we would need to reduce to $\piE{y}{\unittE}{\unittE}$ to know that it is a type constructor.

\begin{figure}[!htb]
  \fbox{$\steps{e}{n}$}
  \begin{mathpar}
    \inferrule*[left=Var$\Downarrow$]
    {
    }
    { \steps{x}{x}
    }

    \inferrule*[left=$\Pi$-I$\Downarrow$]
    {
    }
    { \steps{\lamE{x}{e}}{\lamE{x}{e}}
    }

    \inferrule*[left=$\beta\Downarrow$]
    { \steps{e_f}{\lamE{x}{e_b}} \\
      \steps{\subst{e_b}{x}{e_a}}{n_b}
    }
    { \steps{\appE{e_f}{e_a}}{n_b}
    }

    \inferrule*[left=$\Pi$-E$\Downarrow$]
    { \steps{e_f}{\nu_f} \\
      \steps{e_a}{n_a}
    }
    { \steps{\appE{e_f}{e_a}}{\appE{\nu_f}{n_a}}
    }

    \inferrule*[left=$\Pi$-T$\Downarrow$]
    { \steps{e_d}{n_d}
    }
    { \steps{\piE{x}{n_d}{e_c}}{\piE{x}{n_d}{e_c}}
    }

    \inferrule*[left=$\univE$-T$\Downarrow$]
    {
    }
    { \steps{\univE}{\univE}
    }

    \inferrule*[left=Ann$\Downarrow$]
    { \steps{e}{n}
    }
    { \steps{\annE{e}{e_t}}{n}
    }
  \end{mathpar}
  \caption{Big Step Semantics}
  \label{fig:base-big-step}
\end{figure}

In \autoref{fig:base-big-step} we define the operational semantics for the theory.
In addition to being a semantics for the theory, we can view these rules as a method for turning an expression into its corresponding normal form.

\begin{figure}[!htb]
  \fbox{$\checkJ{\Gamma}{e}{n_t}$} \\
  \fbox{$\synthJ{\Gamma}{e}{n_t}$}
  \begin{mathpar}
    \inferrule*[left=Var]
    { (x : n) \in \Gamma
    }
    { \synthJ{\Gamma}{x}{n}
    }

    \inferrule*[left=Conv]
    { \synthJ{\Gamma}{e}{n_t} \\
      \tyEqJ{\Gamma}{n_t}{n_s}{\univE}
    }
    { \checkJ{\Gamma}{e}{n_s}
    }

    \inferrule*[left=Ann]
    { \checkJ{\Gamma}{e_t}{\univE} \\
      \steps{e_t}{n_t} \\
      \checkJ{\Gamma}{e}{n_t}
    }
    { \synthJ{\Gamma}{\annE{e}{e_t}}{e_t}
    }

    \inferrule*[left=$\Pi$-I]
    { \freshJ{\Gamma}{z} \\
      \steps{\subst{e_c}{y}{z}}{n_c} \\
      \checkJ{\Gamma, z : n_d}{\subst{e}{x}{z}}{n_c}
    }
    { \checkJ{\Gamma}{\lamE{x}{e}}{\piE{y}{n_d}{e_c}}
    }

    \inferrule*[left=$\Pi$-E]
    { \synthJ{\Gamma}{e_f}{\piE{x}{n_d}{e_c}} \\
      \checkJ{\Gamma}{e_a}{n_d} \\
      \steps{\subst{e_c}{x}{e_a}}{n_c}
    }
    { \synthJ{\Gamma}{\appE{e_f}{e_a}}{n_c}
    }

    \inferrule*[left=$\Pi$-T]
    { \checkJ{\Gamma}{e_d}{\univE} \\
      \checkJ{\Gamma, x : e_d}{e_c}{\univE}
    }
    { \checkJ{\Gamma}{\piE{x}{e_d}{e_c}}{\univE}
    }

    \inferrule*[left=$\univE$-in-$\univE$]
    {
    }
    { \checkJ{\Gamma}{\univE}{\univE}
    }
  \end{mathpar}
  \caption{Typing Rules}
  \label{fig:base-typing-rules}
\end{figure}

Finally, we present the typing rules for the theory in a bidirectional style \citep{Dunfield2021}.
The check judgment $\checkJ{\Gamma}{e}{e_t}$ means that the term $e$ checks against the type $n_t$ under the context $\Gamma$.
It is formulated such that it can be decided when given concrete terms for $\Gamma$, $e$, and $n_t$.
The synth judgment $\synthJ{\Gamma}{e}{n_t}$ means that the type $n_t$ can be inferred for the term $e$ under the context $\Gamma$.
It is formulated such that given the concrete terms for $\Gamma$ and $e$, it can be decided if there exists an $n_t$ such that the judgment holds.

Because of the $\univE$-in-$\univE$ rule our theory is inconsistent, but we still add it for two reasons.
First of all, the problem of adding a stratified hierarchy of universes to make the theory consistent has been studied extensively elsewhere and is orthogonal to our concerns.
Second of all, smalltt uses this rule and so adding it ensures our presentation of the theory remains as close to it as possible.

The $\Pi$-I rule uses the $\freshJ{\Gamma}{z}$ judgment to specify that the variable $z$ must not appear in the context $\Gamma$.
It's conventional to assert that variables are fresh in a context, and so we do not define this judgment here.

The Conv rule uses the typed equality judgment $\tyEqJ{\Gamma}{n_a}{n_b}{n_t}$ which we deliberately haven't defined yet.
In the next two subsections we will present two different versions of this judgment: the syntax-directed version which can be decided by only considering $\Gamma$, $n_a$, and $n_b$, and the type-directed version which also needs to consider $n_t$.

\subsection{Syntax Directed Equality}

\begin{figure}[!htb]
  \begin{displaymath}
    \begin{array}{lrcll}
      \text{Scope} & \Theta & \bnfdef & \cdot & \\
      & & \bnfalt & \Theta, x & \\
    \end{array}
  \end{displaymath}
  \fbox{$\tyEqJ{\Gamma}{n_a}{n_b}{n_t}$}
  \fbox{$\stxEqJ{\Theta}{n_a}{n_b}$}
  $\text{strip-types} : \Gamma \to \Theta$ \\
  $\text{strip-types}(\cdot) = \cdot$ \\
  $\text{strip-types}(\Gamma, x : n_t) = \text{strip-types}(\Gamma), x$ \\
  \begin{mathpar}
    \inferrule*[left=Syn]
    { \stxEqJ{\text{strip-types}(\Gamma)}{n_a}{n_b}
    }
    { \tyEqJ{\Gamma}{n_a}{n_b}{n_t}
    }

    \label{rule:stx-eta-l}
    \inferrule*[left=$\eta$-L]
    { \freshJ{\Theta}{y} \\
      \steps{\appE{(\lamE{x}{e_b})}{y}}{n_b} \\
      \steps{\appE{n}{y}}{n_a} \\
      \stxEqJ{\Theta, y}{n_b}{n_a}
    }
    { \stxEqJ{\Theta}{\lamE{x}{e_b}}{n}
    }

    \label{rule:stx-eta-r}
    \inferrule*[left=$\eta$-R]
    { \freshJ{\Theta}{y} \\
      \steps{\appE{(\lamE{x}{e_b})}{y}}{n_b} \\
      \steps{\appE{n}{y}}{n_a} \\
      \stxEqJ{\Theta, y}{n_b}{n_a}
    }
    { \stxEqJ{\Theta}{n}{\lamE{x}{e_b}}
    }

    \label{rule:stx-pi-t}
    \inferrule*[left=$\Pi{=}$]
    { \stxEqJ{\Theta}{n_d}{n_d'} \\
      \freshJ{\Theta}{y} \\
      \steps{\subst{e_c}{x}{y}}{n_c} \\
      \steps{\subst{e_c'}{x'}{y}}{n_c'} \\
      \stxEqJ{\Theta, y}{n_c}{n_c'}
    }
    { \stxEqJ{\Theta}{\piE{x}{n_d}{e_c}}{\piE{x'}{n_d'}{e_c'}}
    }

    \inferrule*[left=$\univE{=}$]
    {
    }
    { \stxEqJ{\Theta}{\univE}{\univE}
    }

    \inferrule*[left=Var${=}$]
    {
    }
    { \stxEqJ{\Theta}{x}{x}
    }

    \inferrule*[left=App${=}$]
    { \stxEqJ{\Theta}{\nu_f}{\nu_f'} \\
      \stxEqJ{\Theta}{n_a}{n_a'}
    }
    { \stxEqJ{\Theta}{\appE{\nu_f}{n_a}}{\appE{\nu_f'}{n_a'}}
    }
  \end{mathpar}
  \caption{Syntax Directed Equality}
  \label{fig:base-syntax-directed-equality}
\end{figure}

In \autoref{fig:base-syntax-directed-equality} we define the typed equality judgment by appealing to the syntax-directed equality judgment $\stxEqJ{\Theta}{n_a}{n_b}$.
The $\pi$=, $\eta$-l, and $\eta$-r rules require adding fresh variables to the context.
However, we don't know what type these new variables should be given in the $\eta$ cases.
Therefore we use $\Theta$ in the syntax-directed equality judgment instead of $\Gamma$, and use strip-types to turn a type context into a scope.

Since we are already given terms in normal form, we don't need to consider $\beta$ equalities until we get to a binding form.
In these cases, specifically the $\pi$=, $\eta$-l, and $\eta$-r rules, we use the big step semantics from \autoref{fig:base-big-step} to reduce all expressions to their normal form before continuing.

In the $\eta$-l and $\eta$-r rules we use a trick to decide the $\eta$ rule for dependent function from only the syntax.
In a normal form, there are only two cases which could be given a function type.
Either the normal form is a function literal, or it is a neutral term.
If both sides of the equality are neutral terms, then applying the $\eta$ rule won't change anything.
Therefore, we only have to apply the function $\eta$ rule in cases where at least one side of the equality is a function literal, leading us to defining our two $\eta$ rules.

\subsection{Type Directed Equality}

\begin{figure}[!htb]
  \fbox{$\tyEqJ{\Gamma}{n_a}{n_b}{n_t}$}
  \fbox{$\chkEqJ{\Gamma}{n_a}{n_b}{n_t}$}
  \fbox{$\synEqJ{\Gamma}{\nu_a}{\nu_b}{n_t}$}

  \begin{mathpar}
    \inferrule*
    { \chkEqJ{\Gamma}{n_a}{n_b}{n_t}
    }
    { \tyEqJ{\Gamma}{n_a}{n_b}{n_t}
    }

    \inferrule*[left=Fun-$\eta$]
    { \freshJ{\Gamma}{y} \\
      \steps{\appE{n_a}{y}}{n_a'} \\
      \steps{\appE{n_b}{y}}{n_b'} \\
      \steps{\subst{e_c}{x}{y}}{n_c} \\
      \chkEqJ{\Gamma, y : n_d}{n_a'}{n_b'}{n_c}
    }
    { \chkEqJ{\Gamma}{n_a}{n_b}{\piE{x}{n_d}{e_c}}
    }

    \inferrule*[left=$\Pi{=}$]
    { \chkEqJ{\Gamma}{n_d}{n_d'}{\univE} \\
      \freshJ{\Gamma}{y} \\
      \steps{\subst{e_c}{x}{y}}{n_c} \\
      \steps{\subst{e_c'}{x'}{y}}{n_c'} \\
      \chkEqJ{\Gamma}{n_c}{n_c'}{\univE}
    }
    { \chkEqJ{\Gamma}{\piE{x}{n_d}{e_c}}{\piE{x'}{n_d'}{e_c'}}{\univE}
    }

    \inferrule*[left=$\univE{=}$]
    {
    }
    { \chkEqJ{\Gamma}{\univE}{\univE}{\univE}
    }

    \inferrule*[left=Conv${=}$]
    { \synEqJ{\Gamma}{\nu_a}{\nu_b}{n_t'}
    }
    { \chkEqJ{\Gamma}{\nu_a}{\nu_b}{n_t}
    }

    \inferrule*[left=Var${=}$]
    { (x : n_t) \in \Gamma
    }
    { \synEqJ{\Gamma}{x}{x}{n_t}
    }

    \inferrule*[left=App${=}$]
    { \synEqJ{\Gamma}{\nu_f}{\nu_f'}{\piE{x}{n_d}{e_c}} \\
      \chkEqJ{\Gamma}{n_a}{n_a'}{n_d} \\
      \steps{\subst{e_c}{x}{n_a}}{n_c}
    }
    { \synEqJ{\Gamma}{\appE{\nu_f}{n_a}}{\appE{\nu_f'}{n_a'}}{n_c}
    }
  \end{mathpar}
  \caption{Type Directed Equality}
  \label{fig:base-type-directed-equality}
\end{figure}

In \autoref{fig:base-type-directed-equality} we define the typed equality judgment in terms of the equality check judgment $\chkEqJ{\Gamma}{n_a}{n_b}{n_t}$ and the equality synth judgment $\synEqJ{\Gamma}{n_a}{n_b}{n_t}$.
The structure of these two judgments closely follows the structure of the synth and check typing judgments from \autoref{fig:base-typing-rules}.
Like the type check judgment, the equality check judgment can be decided when all of $\Gamma$, $n_a$, $n_b$, and $n_t$ are provided.
And similarily, given $\Gamma$, $n_a$, and $n_b$, it can be decided if there exists an $n_t$ such that the equality synth judgment holds.

We use two tricks to decide these rules.
Firsly, we always start comparing normal forms using the equality check judgment.
This way we always have the type we are comparing at available, so we can apply $\eta$ rules based on it.
Secondly, the places in which normal forms appear in neutral terms (e.g. the argument of a function application) correspond to places which use the check typing judgment during type checking.
Therefore, we can infer the type of these neutral terms and then apply the equality check judgment to them, as required by our first trick.

In fact, most of the type-directed equality rules follow the same structure as their corresponding bidirectional typing rules.
For example, the App= rule corresponds to the $\Pi$-E typing rule from \autoref{fig:base-typing-rules} and follows the same structure where synth judgment is applied to the function, and then the check judgment is applied to the argument.
However, the Fun-$\eta$ rule differs from this pattern.
Instead of following the structure of the $\Pi$-I typing rule, it uses the function extensionality principle to convert the problem of equality at the dependent function type into that of equality at the codomain type.

Therefore, we can reuse the Pfenning bidirectional typing recipe \citep{Dunfield2021} to derive the type-directed equality rules.
The only change we make is to replace the introduction rules for types with an $\eta$ rule we want to decide with the $\eta$ rule.

\subsection{Unit and Dependent Pair Types}

Up until this point, the syntax and type-directed equalities have really been the same equality but presented in two different ways.
This is because the $\eta$ rule for dependent functions can be decided both by the syntax and type-directed equalities.
However, once we add the Unit type this changes, since the $\eta$ rule for the unit type is not decidable by the syntax-directed equality.
Additionally, we add the dependent pair type to further illustrate how the type-directed equality rules closely follow from to the bidirectional typing rules.

\begin{figure}[!htb]
  \begin{displaymath}
    \begin{array}{lrcll}
      \text{Expressions} & e & \bnfdef & ... & \\
      & & \bnfalt & \pairE{e_f}{e_s} & \\
      & & \bnfalt & \fstE{e} & \\
      & & \bnfalt & \sndE{e} & \\
      & & \bnfalt & \sigmaE{x}{e_f}{e_s} & \\
      & & \bnfalt & \unitE & \\
      & & \bnfalt & \unittE & \\
      \\
      \text{Normal Forms} & n & \bnfdef & ... & \\
      & & \bnfalt & \pairE{n_f}{n_s} & \\
      & & \bnfalt & \sigmaE{x}{n_f}{e_s} & \\
      & & \bnfalt & \unitE & \\
      & & \bnfalt & \unittE & \\
      \\
      \text{Neutral Terms} & \nu & \bnfdef & ... & \\
      & & \bnfalt & \fstE{\nu} & \\
      & & \bnfalt & \sndE{\nu} & \\
    \end{array}
  \end{displaymath}
  \caption{Unit and Dependent Pair Syntax}
  \label{fig:unit-dependent-pair-syntax}
\end{figure}

In \autoref{fig:unit-dependent-pair-syntax} we present the extension of the syntax with the unit and dependent pair types.
The Unit type has type constructor Unit, and introduction form $\unitE$, while the dependent pair type has type former $\sigmaE{x}{e_f}{e_s}$, introduction form $\pairE{e_f}{e_s}$, and elimination forms $\fstE{e}$ and $\sndE{e}$.

\begin{figure}[!htb]
  \begin{mathpar}
    \inferrule*[left=$\Sigma$-I$\Downarrow$]
    { \steps{e_f}{n_f} \\
      \steps{e_s}{n_s} \\
    }
    { \steps{\pairE{e_f}{e_s}}{\pairE{n_f}{n_s}}
    }

    \inferrule*[left=$\sigma_1$]
    { \steps{e}{\pairE{n_f}{n_s}} \\
    }
    { \steps{\fstE{e}}{n_f}
    }

    \inferrule*[left=$\sigma_2$]
    { \steps{e}{\pairE{n_f}{n_s}} \\
    }
    { \steps{\sndE{e}}{n_s}
    }

    \inferrule*[left=$\sigma$-I1$\Downarrow$]
    { \steps{e}{\nu}
    }
    { \steps{\fstE{e}}{\fstE{\nu}}
    }

    \inferrule*[left=$\sigma$-I2$\Downarrow$]
    { \steps{e}{\nu}
    }
    { \steps{\sndE{e}}{\sndE{\nu}}
    }

    \inferrule*[left=$\sigma$-T$\Downarrow$]
    { \steps{e_1}{n_1}
    }
    { \steps{\sigmaE{x}{e_1}{e_2}}{\sigmaE{x}{n_1}{e_2}}
    }

    \inferrule*[left=$\unittE$-I$\Downarrow$]
    {
    }
    { \steps{\unitE}{\unitE}
    }

    \inferrule*[left=$\unittE$-T$\Downarrow$]
    {
    }
    { \steps{\unittE}{\unittE}
    }
  \end{mathpar}
  \caption{Unit and Dependent Pair Semantics}
  \label{fig:unit-dependent-pair-semantics}
\end{figure}
The semantics presented for the unit and dependent pairs in \autoref{fig:unit-dependent-pair-semantics} is completely standard, as are the typing rules presented in \autoref{fig:unit-dependent-pair-typing}.
\begin{figure}[!htb]
  \begin{mathpar}
    \inferrule*[left=$\Sigma$-I]
    { \checkJ{\Gamma}{e_f}{n_1} \\
      \steps{\subst{e_2}{x}{e_s}}{n_2} \\
      \checkJ{\Gamma}{e_s}{n_2}
    }
    { \checkJ{\Gamma}{\pairE{e_f}{e_s}}{\sigmaE{x}{n_1}{e_2}}
    }

    \inferrule*[left=$\Sigma$-E1]
    { \synthJ{\Gamma}{e}{\sigmaE{x}{n_1}{e_2}} \\
    }
    { \synthJ{\Gamma}{\fstE{e}}{n_1}
    }

    \inferrule*[left=$\Sigma$-E2]
    { \synthJ{\Gamma}{e}{\sigmaE{x}{n_1}{e_2}} \\
      \steps{\subst{e_2}{x}{\fstE{e}}}{n_2}
    }
    { \synthJ{\Gamma}{\sndE{e}}{n_2}
    }

    \inferrule*[left=$\Sigma$-T]
    { \checkJ{\Gamma}{e_1}{\univE} \\
      \steps{e_1}{n_1} \\
      \checkJ{\Gamma, x : n_1}{e_2}{\univE} \\
    }
    { \checkJ{\Gamma}{\sigmaE{x}{e_1}{e_2}}{\univE}
    }

    \inferrule*[left=$\unittE$-I]
    {
    }
    { \checkJ{\Gamma}{\unitE}{\unittE}
    }

    \inferrule*[left=$\unittE$-T]
    {
    }
    { \checkJ{\Gamma}{\unittE}{\univE}
    }
  \end{mathpar}
  \caption{Unit and Dependent Pair Typing}
  \label{fig:unit-dependent-pair-typing}
\end{figure}

\begin{figure}[!htb]
  \begin{mathpar}
    \inferrule*[left=$\Sigma$-$\eta$-L]
    { \steps{\fstE{n}}{n_f'} \\
      \steps{\sndE{n}}{n_s'} \\
      \stxEqJ{\Theta}{n_f}{n_f'} \\
      \stxEqJ{\Theta}{n_s}{n_s'}
    }
    { \stxEqJ{\Theta}{\pairE{n_f}{n_s}}{n}
    }

    \inferrule*[left=$\Sigma$-$\eta$-R]
    { \steps{\fstE{n}}{n_f'} \\
      \steps{\sndE{n}}{n_s'} \\
      \stxEqJ{\Theta}{n_f}{n_f'} \\
      \stxEqJ{\Theta}{n_s}{n_s'}
    }
    { \stxEqJ{\Theta}{n}{\pairE{n_f}{n_s}}
    }

    \inferrule*[left=$\Sigma$-T${=}$]
    { \stxEqJ{\Theta}{n_1}{n_1'} \\
      \freshJ{\Theta}{y} \\
      \steps{\subst{e_2}{x}{y}}{n_2} \\
      \steps{\subst{e_2'}{x'}{y}}{n_2'} \\
      \stxEqJ{\Theta}{n_2}{n_2'}
    }
    { \stxEqJ{\Theta}{\sigmaE{x}{n_1}{e_2}}{\sigmaE{x'}{n_1'}{e_2'}}
    }

    \inferrule*[left=$\Sigma$-E1${=}$]
    { \stxEqJ{\Theta}{\nu}{\nu'}
    }
    { \stxEqJ{\Theta}{\fstE{\nu}}{\fstE{\nu'}}
    }

    \inferrule*[left=$\Sigma$-E2${=}$]
    { \stxEqJ{\Theta}{\nu}{\nu'}
    }
    { \stxEqJ{\Theta}{\sndE{\nu}}{\sndE{\nu'}}
    }

    \inferrule*[left=$\unittE$-I${=}$]
    {
    }
    { \stxEqJ{\Theta}{\unitE}{\unitE}
    }

    \inferrule*[left=$\unittE$-T${=}$]
    {
    }
    { \stxEqJ{\Theta}{\unittE}{\unittE}
    }
  \end{mathpar}
  \caption{Unit and Dependent Pair Syntax Directed Equality}
  \label{fig:unit-dependent-pair-syntax-directed-equality}
\end{figure}

In \autoref{fig:unit-dependent-pair-syntax-directed-equality} we present the syntax-directed equality for the unit and dependent pairs.
We use a trick to decide the $\eta$ rule for dependent pairs similar to that used to syntactially decide the $\eta$ rule for dependent functions.
However, the same trick doesn't work for the unit $\eta$ rule.
In the case of dependent functions and pairs, if the terms being compare were both neutral, then applying the corresponding $\eta$ rule doesn't make a difference.
But, for the unit type it does make a difference, since we can judge any two neutrals equal using the unit $\eta$ rule.
Therefore, in general, we have to have the type of the two terms available to know when we should apply the unit $\eta$ rule.

\begin{figure}[!htb]
  \begin{mathpar}
    \inferrule*[left=$\Sigma$-$\eta$]
    { \steps{\fstE{n}}{n_f} \\
      \steps{\fstE{n'}}{n_f'} \\
      \chkEqJ{\Gamma}{n_f}{n_f'}{n_1} \\
      \steps{\subst{e_2}{x}{n_f}}{n_2} \\
      \steps{\sndE{n}}{n_s} \\
      \steps{\sndE{n'}}{n_s'} \\
      \chkEqJ{\Gamma}{n_s}{n_s'}{n_2}
    }
    { \chkEqJ{\Gamma}{n}{n'}{\sigmaE{x}{n_1}{e_2}}
    }

    \inferrule*[left=$\Sigma$-T${=}$]
    { \chkEqJ{\Gamma}{n_1}{n_1'}{\univE} \\
      \freshJ{\Gamma}{y} \\
      \steps{\subst{e_2}{x}{y}}{n_2} \\
      \steps{\subst{e_2'}{x'}{y}}{n_2'} \\
      \chkEqJ{\Gamma}{n_2}{n_2'}{\univE}
    }
    { \chkEqJ{\Gamma}{\sigmaE{x}{n_1}{e_2}}{\sigmaE{x'}{n_1'}{e_2'}}{\univE}
    }

    \inferrule*[left=$\Sigma$-E1${=}$]
    { \synEqJ{\Gamma}{\nu}{\nu'}{\sigmaE{x}{n_1}{e_2}}
    }
    { \synEqJ{\Gamma}{\fstE{\nu}}{\fstE{\nu'}}{n_1}
    }

    \inferrule*[left=$\Sigma$-E2${=}$]
    { \synEqJ{\Gamma}{\nu}{\nu'}{\sigmaE{x}{n_1}{e_2}} \\
      \steps{\subst{e_2}{x}{\fstE{\nu}}}{n_2}
    }
    { \synEqJ{\Gamma}{\sndE{\nu}}{\sndE{\nu'}}{n_2}
    }

    \inferrule*[left=$\unittE$-$\eta$]
    {
    }
    { \chkEqJ{\Gamma}{n}{n'}{\unittE}
    }

    \inferrule*[left=$\unittE$-T${=}$]
    {
    }
    { \chkEqJ{\Gamma}{\unittE}{\unittE}{\univE}
    }
  \end{mathpar}
  \caption{Unit and Dependent Pair Type Directed Equality}
  \label{fig:unit-dependent-pair-type-directed-equality}
\end{figure}

Finally, in \autoref{fig:unit-dependent-pair-type-directed-equality} we present the type-directed equality rules for the unit and dependent pair types.
Here we decide the unit and dependent pair $\eta$ rules in much the same way as we do for the dependent function type.
If the type we are comparing at is a unit or dependent pair, simply apply the corresponding $\eta$ and continue.

\section{Methods}

\subsection{\textit{smalltt} Implementation}

% To evaluate the relative performance of the syntax and type directed equalities we benchmarkeded two versions of the same type checker, one with the syntax directed equality, and one with the type directed equality.
% Andras Kovac's smalltt \footnote{\href{https://github.com/AndrasKovacs/smalltt}{https://github.com/AndrasKovacs/smalltt}} is a type checker for a dependent type theory quite similar to the one presented here.
% It already uses the syntax direct equality, so we created a modified version which only differs in that it uses the type directed equality.
% It also
% 
% \begin{quote}
% \textit{Disclaimer}: smalltt includes metavariables for inferring parts of the program.
% This means that the code for deciding judgmental equality must also implement unification for solving these metavariables.
% Since this is a separate concern from the method for deciding judgmental equality, the code presented here will omit the cases where metavariables must be dealt with.
% \end{quote}

To evaluate the performance and utility of the recipe for type-directed judgmental equality, we have produced two modified versions of Andras Kovac's smalltt.
The first version has the same syntax and semantics as the original smalltt, but uses a type-directed judgmental equality.
The goal of this version is to evaluate the performance of the recipe as compared to the original smalltt type checker.
The second version extends the first modification with the unit and dependent pair types.
The primary goal of this version is to demonstrate that the type-directed equality can decide the unit $\eta$ rule.
Secondarily, it demonstrates the ease of adding new constructs to the language by following the bidirectional recipe.

The language in the original version of smalltt is almost identical to that presented in \autoref{fig:base-syntax}.
It only differs from the theory considered here in two ways.
Firstly, it includes metavariables, essentially holes in a term which are inferred via unification during type checking.
Secondly, it uses an optimization while deciding judgmental equality to try and prematurely decide that the two terms are equal.
This optimization exploits the fact that judgmental equality is a congruence relation with respect to substitution.
That is to say that if $\tyEqJ{\Gamma}{n_a}{n_b}{n_t}$, then we can infer that $\tyEqJ{\Gamma}{n_a'}{n_b'}{n_t}$ where $\steps{\subst{n_a}{x}{e}}{n_a'}$ and $\steps{\subst{n_b}{x}{e}}{n_b'}$.
Therefore, if we can detect scenarios where the same substitution is being applied to both side of an equality judgment, we can first check if the terms are equal pre-substitution.
This optimization is of course predicated on the assumption that $n_a'$ and $n_b'$ will usually be substantially larger than $n_a$ and $n_b$.
Neither of these should affect our performance comparison since they are present in both the original and our modified versions of smalltt.

The original version of smalltt is available at \url{https://github.com/AndrasKovacs/smalltt/}, while our two modified versions are available at \url{https://github.com/ChesterJFGould/smalltt} on the master and sigma-unit branches respectively.

\subsubsection{Modifying \textit{smalltt}}

Here we will present an overview of the part of the original smalltt which decides judgmental equality, and then present the overall changes we made to use a type-directed equality.
Since inferring values for meta variables is irrelevant to the core of this thesis, we have simplified the code shown here slightly to remove most of those parts.
The core data types and functions from the original smalltt are as follows.

\newpage

\begin{lstlisting}

data Tm
  = LocalVar Ix
  | App Tm Tm Icit
  | Lam NameIcit Tm
  | Pi NameIcit Ty Ty
  | U

type Ty = Tm

type Ix = Int

type Lvl = Int

data Val =
    VLocalVar Lvl Spine
  | VLam NameIcit Closure
  | VPi NameIcit VTy Closure
  | VU

type VTy = Val

data Spine =
    SId
  | SApp Spine Val

data Closure = Closure Env Tm

data Env =
    ENil
  | EDef Env Val

eval :: Env -> Tm -> Val

unify :: Lvl -> Val -> Val -> IO ()

unifySp :: Lvl -> Spine -> Spine -> IO ()
\end{lstlisting}

Here the \lstinline{Tm} type corresponds to our expressions, \lstinline{Val} to normal forms, and \lstinline{Spine} to the neutral terms.
Note that the \lstinline{Val} type uses deBruijn levels to represent variables instead of names.
For example, with deBruijn levels the term $\lambda x. \lambda y. x$ would be represented as $\lambda. \lambda. 0$.
The \lstinline{Env} type represented a group of substitutions, and so the \lstinline{eval} function corresponds to a combination of the $\steps{e}{n}$ relation and the $\subst{e}{x}{e'}$ function.
Additionally, a \lstinline{Closure} representents a \lstinline{Env}
Finally, \lstinline{unify} and \lstinline{unifySp} correspond to the $\stxEqJ{\Theta}{n_a}{n_b}$ judgment, but they use the current deBruijn level to generate fresh variables instead of $\Theta$.
Both \lstinline{unify} and \lstinline{unifySp} return an \lstinline{IO ()} since they will throw an exception if the terms aren't equal or evaluate to \lstinline{return ()} if they are.

\newpage

Next we present the modifications made to convert smalltt to using a type-directed equality.

\begin{lstlisting}
unifyChk :: Cxt -> Val -> Val -> VTy -> IO ()

unifySp :: Cxt -> VTy -> Spine -> Spine -> IO VTy

type TypeCxt = Map Lvl VTy

type Cxt = Cxt {lvl :: Lvl, localTypes :: TypeCxt}
\end{lstlisting}

We only needed to change the type of \lstinline{unifyChk} and \lstinline{unifySp}.
\lstinline{unifyChk} now correponds to the $\chkEqJ{\Gamma}{n_a}{n_b}{n_t}$ judgment, while \lstinline{unifySp} now correponds to the $\synEqJ{\Gamma}{n_a}{n_b}{n_t}$ judgment.

Finally we present the modifications made to add the unit and dependent pair types to our type-directed version of smalltt.

%% The input \lstinline{Tm} type represents a well typed expression where all of the local variables have been replaced with deBruijn indices, and the global variables with deBruijn levels \footnote{deBruijn indices replace variables with the number of binders between them and their binding site, e.g. $\lambda x. \lambda y. x$ becomes $\lambda. \lambda. 1$. Whereas deBruijn levels do the same, but number the binding sites starting at the outermost binding, e.g. $\lambda x. \lambda y. x$ becomes $\lambda. \lambda. 0$}.
%% \lstinline{Val}, the output of \lstinline{eval}, is a datatype representing an expression in $\beta$-normal form with both local global variables represented as deBruijn levels.
%% Since there's no need to modify this part of the code, I won't discuss the implementation of the \lstinline{eval} function.
%% The standard reference for this algorithm is \citet{Coquand1996}.
%% 
%% The second phase is where the bulk of the changes lie.
%% The original functions implementing this phase were approximately the following.

\begin{lstlisting}
data Tm
  = LocalVar Ix
  | App Tm Tm Icit
  | Lam NameIcit Tm
  | Pi NameIcit Ty Ty
  | Sigma NameIcit Ty Ty
  | SigmaI Tm Tm
  | Fst Tm
  | Snd Tm
  | Unit
  | UnitI
  | U

data Val =
    VLocalVar Lvl Spine
  | VLam NameIcit Closure
  | VPi NameIcit VTy Closure
  | VSigma NameIcit VTy Closure
  | VSigmaI Val Val
  | VUnit
  | VUnitI
  | VU

data Spine =
    SId
  | SApp Spine Val
  | SFst Spine
  | SSnd Spine
\end{lstlisting}

Here we didn't need to change the type of any functions, just add the new cases to the existing datatypes and functions.

%% These check syntactic equality on values and spines, respectively, with $\eta$-rules for functions also being applied.
%% They take the current deBruijn level as an additional parameter for when a new local variable needs to be introduced.
%% If the values are equal, they will complete succesfully with a value of \lstinline{()}.
%% Otherwise, they will throw an exception.
%% 
%% In line with the method presented in \citet{Chapman2005}, these functions were re-written as the following.
%% 
%% \begin{lstlisting}
%% unifyTy :: Cxt -> VTy -> VTy -> IO ()
%% 
%% unifyChk :: Cxt -> Val -> Val -> VTy -> IO ()
%% 
%% unifySp :: Cxt -> VTy -> Spine -> Spine -> IO VTy
%% 
%% type TypeCxt = Map Lvl VTy
%% 
%% type Cxt = Cxt {lvl :: Lvl, localTypes :: TypeCxt, globalTypes :: TypeCxt}
%% \end{lstlisting}
%% 
%% Here the \lstinline{unify} function has been split into two parts, one for checking judgmental equality between types, and one for checking it between terms at a specific type.
%% However, since we have the impredicative type-in-type, we can uniquely determine \lstinline{unifyTy} by the following equation.
%% 
%% \begin{lstlisting}
%% unifyTy cxt a b = unifyChk cxt a b VU
%% \end{lstlisting}
%% 
%% Here, the only $\eta$ rule we have to apply is the the rule for dependent functions which takes the following form.
%% 
%% \begin{mathpar}
%% \inferrule*[left={$x$ fresh}]
%%   {\tyEqJ{\Gamma, x : D}{\appE{f}{x}}{\appE{g}{x}}{\subst{C}{y}{x}}}
%%   {\tyEqJ{\Gamma}{f}{g}{\piE{y}{D}{C}}}
%% \end{mathpar}
%% 
%% To handle it we add the following equation for \lstinline{unifyChk}
%% 
%% \begin{lstlisting}
%% unifyChk cxt f g (VPi d c) = unifyChk cxt' (doApp f x) (doApp g x) (appCl c x)
%%   where (cxt', x) = cxtNewLocal d cxt
%% \end{lstlisting}
%% 
%% Here we use two new helpers, \lstinline{doApp} which applies the first argument to the second and $\beta$-reduces if necessary, and \lstinline{appCl} which substitutes its second argument into the body of its first.
%% 
%% \begin{lstlisting}
%% doApp :: Val -> Val -> Val
%% 
%% appCl :: Closure -> Val -> Val
%% \end{lstlisting}
%% 
%% Next we have the equations for the constructors of types which either don't have $\eta$-rules, or don't have decidable $\eta$-rules.
%% In our case this is the type of types.
%% 
%% \begin{lstlisting}
%% unifyChk cxt (VPi d c) (VPi d' c') VU = unifyChk cxt d d' VU >> unifyChk cxt' (appCl c x) (appCl' c' x) VU
%%   where (cxt', x) = cxtNewLocal d cxt
%% 
%% unifyChk cxt VU VU VU = return ()
%% \end{lstlisting}
%% 
%% The last chance our two values have to be equal is if they are a variables with a spine.
%% 
%% \begin{lstlisting}
%% unifyChk cxt (VLocalVar x sp) (VLocalVar x' sp') _
%%   | x == x' = unifySp cxt sp sp' >> return ()
%%   | otherwise = throw (UnifyEx Conversion)
%% 
%% unifyChk cxt (VTopVar _ _ v) (VTopVar _ _ v') t = unifyChk cxt v v' t
%% \end{lstlisting}
%% 
%% Additionally, the \lstinline{unifySp} function not only needs to check two spines for equality, but also return their type if they are equal.
%% Since the spines themselves don't contain the variable being applied, 
%% 
%% \subsubsection{Extending \textit{smalltt}}
%% 
%% To extend version of smalltt presented in section 4.1 with the unit and dependent pair types, I first modified the syntax of both terms and values.
%% 
%% \begin{lstlisting}
%% data Tm
%%   = ...
%%   | Sigma NameIcit Ty Ty
%%   | SigmaI Tm Tm
%%   | Fst Tm
%%   | Snd Tm
%%   | Unit
%%   | UnitI
%% \end{lstlisting}


\subsection{Data Analysis}

\subsubsection{The Problem}

Programs don't have deterministic runtimes, but their runtimes aren't completely random either.
We can model each run of a program as being drawn from some probability distribution, and so benchmarking a program can be viewed as sampling that program's runtime distribution.

In the case considered here, where we want to know if the runtime of the modified type checker performs as well as the original, the question becomes ``is the mean of the runtime distribution of the modified type checker greater than that of the original program?".
To answer this question, we can only use the runtime sample for each implementation collected during benchmarking.
But, this begs the question, how can we use these samples to answer the question?
And how large should the samples be?

\subsubsection{Welch's T-Test}

There are a few different methods we could use to compare the runtime distribution means of each implementation, the most popular probably being Student's t-test.
This test would let us estimate the probability that we get the observed benchmarking results under the assumption that the runtime distribution mean of the modified type checker \textit{isn't} larger that the original.
If the probability is low, then we can say that it is likely that the modified type checker is slower than the original.

However, Student's t-test also assumes both that the variance in distribution of sample means for both type checkers are the same, and that they are normally distributed.
The later is not an unreasonable assumption.
The central limit theorem tells us that the distribution of sample means approaches a normal distribution as sample size increases.
However, we have no reason to assume that the variances are equal, and so the first assumption is unreasonable.
Therefore we use a generalization of Student's t-test called Welch's t-test which does not make this assumption \citep{Welch1947}.

\subsubsection{Sample Size, Power, and Effect Size}

To determine what sample size to use, we need to decide on how powerful to make our statistical test.
In this case, the power is the probability of deciding that the modified type checker is slower under the assumption that it actually is.
This also determines the probability of deciding that the modified type checker isn't slower even when it is, a situation we would like to avoid.
Therefore we set the power far higher than the standard $0.8$, setting it at $0.99$, giving ourselves a 1\% chance of a false positive.
We also set the significance level, the probability of deciding that the modified type checker is slower when it isn't, to the similar value of $0.01$.
Additionally we set the minimum detectable effect size to $0.5$, meaning that the runtime distributions must differ by at least $0.5$ standard deviations for us to detect a difference between them.

To achieve these values, the G*Power tool \citep{Faul2009} calculates that we need to run each benchmark 175 times per implementation.

\subsubsection{Method}
To collect the samples, we ran each smalltt benchmark 175 times using the hyperfine \footnote{\url{https://github.com/sharkdp/hyperfine}} tool.
These benchmarks were run on a Intel Skylake Xeon running at 2.5GHz with 120G of ram.
We then analyzed the collected data using the statistics Haskell library \footnote{\url{https://hackage.haskell.org/package/statistics}}.
Our raw benchmark results and the code we used to analyze them are available at \url{https://github.com/ChesterJFGould/HonoursThesis}.

\section{Results and Conclusion}

\begin{figure}[!htb]
  \centering
  \begin{tabular}{|c | c|}
    \hline
    Benchmark & P-Value
    \\ \hline
    asymptotics & $1.5 \times 10^{-161}$
    \\ \hline
    conv\_eval & $9.9 \times 10^{-261}$
    \\ \hline
    stlc & $8.2 \times 10^{-309}$
    \\ \hline
    stlc5k & $3.6 \times 10^{-305}$
    \\ \hline
    stlc10k & DNF
    \\ \hline
    stlc100k & DNF
    \\ \hline
    stlc\_lessimpl & $4.1 \times 10^{-311}$
    \\ \hline
    stlc\_lessimpl5k & $0$
    \\ \hline
    stlc\_lessimpl10k & DNF
    \\ \hline
    stlc\_small & $0$
    \\ \hline
    stlc\_small10k & $2.6 \times 10^{-284}$
    \\ \hline
    stlc\_small5k & $1.8 \times 10^{-301}$
    \\ \hline
  \end{tabular}
  \caption{Benchmark Results}
  \label{fig:results}
\end{figure}

In \autoref{fig:results} we can see the results of the performance comparison between the original syntax-directed smalltt and our modified type-directed smalltt.
The p-value is the probability that we got the observed benchmark results under the assumption that our modified version smalltt isn't slower than the original.
Across the board this probability is very small, and so it seems likely that our modified version of smalltt is slower than the original.
That being said, a few cases jump out in this table.

In the stlc\_lessimpl5k and stlc\_small benchmarks, the p-value is 0.
We interpret this as a p-value so small that it can't be represented as a double-precision floating point number, since these are what we used to do the statistical test.

Our modified version of smalltt did not finish the stlc10k, stlc100k, and stlc\_lessimpl10k benchmarks due to running out of memory.
That our implementation failed to complete some of the benchmarks isn't entirely surprising.
smalltt includes equivalent benchmarks for some other common theorem provers such as Agda, Coq, and Idris2.
In the performance comparison given in the README for smalltt, both Agda and Idris2 also failed to finish on the stlc100k.

This, however, doesn't detract from the fact that the original smalltt is clearly faster than our implementation.
Therefore, we can reject our thesis statement and say that while the type-directed equality supports more rules than the syntax-directed equality, the syntax-directed equality performs better.

\section{Future Work}

After conducting this research some questions remain.

\subsection{Is the unit $\eta$ rule the only rule which requires a type-directed equality?}

In our presented theory, the only judgmental equality rule which required the type-directed approach was the unit $\eta$ rule.
\citet{altenkirch2001} present an algorithm for deciding the $\eta$ rule for the coproduct type.
However, it's unclear if their algorithm would fit with the type-directed approach we present here.
\citet{gilbert2019} present a type theory with a universe of definitionally irrelevant propositions.
While similar to the unit $\eta$ rule, their $\eta$ rule for propositions requires not only the type of the two terms be known, but also the type of the type.

\subsection{Could our implementation be optimized?}

While our smalltt implementation as is failed to perform as well as the syntax-directed smalltt, it's not out of the question that it could be optimized further.
We leave this optimization as future work.

\subsection{Was our method of correcting for variation in performance due to memory layout effective?}

Previous work has shown that a program's memory layout can drastically affect performance, with even small things like the values of environment variables affecting this layout.
We attempted to correct for this in our statistical test, but it's unclear if we did this properly.
Properly developing statistical methods to solve this problem would be very useful when running any sort of performance comparison between programs.

\newpage

\bibliography{main}

\end{document}
